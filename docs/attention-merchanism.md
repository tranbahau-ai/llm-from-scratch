CÆ¡ cháº¿ attention giÃºp mÃ´ hÃ¬nh **táº­p trung vÃ o cÃ¡c pháº§n quan trá»ng** cá»§a Ä‘áº§u vÃ o khi táº¡o Ä‘áº§u ra. Thay vÃ¬ xá»­ lÃ½ táº¥t cáº£ token nhÆ° nhau, attention **há»c xem token nÃ o quan trá»ng hÆ¡n** táº¡i má»—i thá»i Ä‘iá»ƒm.

**Attention** tÃ­nh Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng giá»¯a:
- Query (Q)
- Key (K)  
    â†’ Äiá»ƒm cÃ ng cao â†’ token Ä‘Ã³ cÃ ng quan trá»ng  
    Äiá»ƒm nÃ y sáº½ dÃ¹ng Ä‘á»ƒ káº¿t há»£p Value (V) táº¡o ra **Ä‘áº¡i diá»‡n Ä‘áº§u ra**.

### CÃ´ng thá»©c tá»•ng quÃ¡t / Foundation 

$$
Attention(Q,K,V) = \text{softmax} (\frac{Q \cdot K^\top}{\sqrt{d_k}}) * V
$$
Trong Ä‘Ã³:
 **Query (Q) :** Token Ä‘ang Ä‘áº·t cÃ¢u há»i
 **Key (K):** Token chá»©a thÃ´ng tin
 **Value (V):** ThÃ´ng tin thá»±c sá»± cáº§n láº¥y
 **softmax:**	TÃ­nh má»©c Ä‘á»™ â€œnÃªn táº­p trung vÃ o token nÃ o nháº¥tâ€

### ğŸ§  Detail explaination:
**ğŸ“ 1. Dot Product (TÃ­ch vÃ´ hÆ°á»›ng)**

$$
\text{score} = Q \cdot K^\top
$$

**Example:**

$$
Q = [1, 2, 3], \quad K = [2, 1, 0]
$$
$$
Q \cdot K = 1\times2 + 2\times1 + 3\times0 = 4
$$
$$
\text{score} = Q \cdot K^\top
$$
**Q** = â†“ hÆ°á»›ng cáº§n tÃ¬m (cÃ¢u há»i)  
**K** = dá»¯ liá»‡u cÃ³ sáºµn (cÃ¡c Ä‘Ã¡p Ã¡n)
`Q Â· K.T` = **Ä‘á»™ giá»‘ng nhau giá»¯a cÃ¢u há»i vÃ  tá»«ng Ä‘Ã¡p Ã¡n**
â†’ Náº¿u **Q & K** hÆ°á»›ng giá»‘ng nhau â†’ score cao (relevant)  
â†’ Náº¿u khÃ¡c hÆ°á»›ng â†’ score tháº¥p (not relevant)

####  Code Example (PyTorch)

```python
import torch

Q = torch.tensor([[1.0, 2.0, 3.0]])  # (1 x 3)
K = torch.tensor([[2.0, 1.0, 0.0],
                  [1.0, 2.0, 3.0]])  # (2 x 3)

# compute similarity
score = torch.matmul(Q, K.T)
print(score)  # tensor([[4., 14.]])
```

**ğŸ§  2. Scaling (Stabilization/TÃ­nh á»•n Ä‘á»‹nh)**
To prevent large values when vector dimension is big:

$$
\text{scores} = \frac{Q \cdot K^\top}{\sqrt{d_k}}
$$
**LÃ­ do:**
Náº¿u sá»‘ chiá»u cá»§a vector lá»›n â†’ Dot product quÃ¡ cao â†’ gradient exploded â†’ model há»c khÃ´ng á»•n Ä‘á»‹nh.  
Chia cho $\sqrt{d_k}$ â€‹ Ä‘á»ƒ **á»•n Ä‘á»‹nh training**.

**ğŸ§  3.  Softmax**
$$
weights=softmax(scores)
$$
**biáº¿n cÃ¡c raw scores** thÃ nh **xÃ¡c suáº¥t** (giÃ¡ trá»‹ tá»« 0 Ä‘áº¿n 1 vÃ  tá»•ng = 1).
=> mÃ´ hÃ¬nh biáº¿t nÃªn chÃº Ã½ bao nhiÃªu % vÃ o tá»«ng token.

**ğŸ§  4.  Final Step -> nhÃ¢n trá»ng sá»‘ Ä‘Ã³ vá»›i V** 

$\text{scores} = \frac{Q \cdot K^\top}{\sqrt{d_k}}$  ->  $\text{weights}=softmax(scores)$  ->  $\text{output}=weights * V$

**nhÃ¢n trá»ng sá»‘ Ä‘Ã³ vá»›i V** â€” tá»©c lÃ  **thÃ´ng tin tháº­t sá»± cáº§n láº¥y**.

### ğŸ§  **Causal Attention (dÃ¹ng trong GPT)**
**DÃ¹ng Ä‘á»ƒ Ä‘áº£m báº£o chá»‰ nhÃ¬n token phÃ­a trÆ°á»›c, khÃ´ng nhÃ¬n tÆ°Æ¡ng lai**  
MÃ´ hÃ¬nh chá»‰ Ä‘Æ°á»£c phÃ©p â€œdá»± Ä‘oÃ¡n tá»«ng token má»™tâ€ â†’ **token by token generation**
Masking giÃºp che nhá»¯ng token chÆ°a xuáº¥t hiá»‡n

$$
\frac{Q \cdot K^\top}{\sqrt{d_k}} + \text{mask}
$$

### Tá»•ng káº¿t
Attention giÃºp mÃ´ hÃ¬nh **táº­p trung, suy luáº­n vÃ  linh hoáº¡t**. ÄÃ¢y lÃ  **core cá»§a Transformer & GPT**.