## C∆° ch·∫ø attention

C∆° ch·∫ø attention gi√∫p m√¥ h√¨nh **t·∫≠p trung v√†o c√°c ph·∫ßn quan tr·ªçng** c·ªßa ƒë·∫ßu v√†o khi t·∫°o ƒë·∫ßu ra. Thay v√¨ x·ª≠ l√Ω t·∫•t c·∫£ token nh∆∞ nhau, attention **h·ªçc xem token n√†o quan tr·ªçng h∆°n** t·∫°i m·ªói th·ªùi ƒëi·ªÉm.

**Attention** t√≠nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng gi·ªØa:
- Query (Q)
- Key (K)  
    ‚Üí ƒêi·ªÉm c√†ng cao ‚Üí token ƒë√≥ c√†ng quan tr·ªçng  
    ƒêi·ªÉm n√†y s·∫Ω d√πng ƒë·ªÉ k·∫øt h·ª£p Value (V) t·∫°o ra **ƒë·∫°i di·ªán ƒë·∫ßu ra**.

### C√¥ng th·ª©c t·ªïng qu√°t / Self-Attention 

$$
Attention(Q,K,V) = \text{softmax} (\frac{Q \cdot K^\top}{\sqrt{d_k}}) * V
$$

Trong ƒë√≥:
 **Query (Q) :** Token ƒëang ƒë·∫∑t c√¢u h·ªèi
 **Key (K):**    Token ch·ª©a th√¥ng tin
 **Value (V):**  Th√¥ng tin th·ª±c s·ª± c·∫ßn l·∫•y
${\sqrt{d_k}}$: dimension c·ªßa key (ƒë·ªÉ scaling)

 **softmax:**	 T√≠nh m·ª©c ƒë·ªô ‚Äún√™n t·∫≠p trung v√†o token n√†o nh·∫•t‚Äù

### üß† Detail explaination:
**üìê 1. Dot Product (T√≠ch v√¥ h∆∞·ªõng)**

$$
\text{score} = Q \cdot K^\top
$$

**Example:**

$$
Q = [1, 2, 3], \quad K = [2, 1, 0]
$$
$$
Q \cdot K = 1\times2 + 2\times1 + 3\times0 = 4
$$

**Q** = ‚Üì h∆∞·ªõng c·∫ßn t√¨m (c√¢u h·ªèi)  
**K** = d·ªØ li·ªáu c√≥ s·∫µn (c√°c ƒë√°p √°n)
Q ¬∑ K.T = ƒë·ªô gi·ªëng nhau gi·ªØa c√¢u h·ªèi v√† t·ª´ng ƒë√°p √°n
‚Üí N·∫øu **Q & K** h∆∞·ªõng gi·ªëng nhau ‚Üí score cao (relevant)  
‚Üí N·∫øu kh√°c h∆∞·ªõng ‚Üí score th·∫•p (not relevant)

####  Code Example (PyTorch)

```python
import torch

Q = torch.tensor([[1.0, 2.0, 3.0]])  # (1 x 3)
K = torch.tensor([[2.0, 1.0, 0.0],
                  [1.0, 2.0, 3.0]])  # (2 x 3)

# compute similarity
score = torch.matmul(Q, K.T)
print(score)  # tensor([[4., 14.]])
```

**üß† 2. Scaling (Stabilization/T√≠nh ·ªïn ƒë·ªãnh)**
To prevent large values when vector dimension is big:

$$
\text{scores} = \frac{Q \cdot K^\top}{\sqrt{d_k}}
$$

**L√≠ do:**
N·∫øu s·ªë chi·ªÅu c·ªßa vector l·ªõn ‚Üí Dot product qu√° cao ‚Üí gradient exploded ‚Üí model h·ªçc kh√¥ng ·ªïn ƒë·ªãnh.  
Chia cho $\sqrt{d_k}$ ‚Äã ƒë·ªÉ **·ªïn ƒë·ªãnh training**.

**üß† 3.  Softmax**
$$
weights=softmax(scores)
$$

‚Üí Softmax bi·∫øn ƒë·ªô li√™n quan th√†nh **tr·ªçng s·ªë x√°c su·∫•t**


**üß† 4.  Final Step** 

$\text{scores} = \frac{Q \cdot K^\top}{\sqrt{d_k}}$  ->  $\text{weights}=softmax(scores)$  ->  $\text{output}=weights * V$

‚Üí Multiply v·ªõi V ƒë·ªÉ l·∫•y **contextual embedding**


##### Code example for Simple Self-Attention (no training)


```python
import torch
import torch.nn.functional as F

# Example input (T tokens, each 3-dim embedding)
X = torch.tensor([
    [0.4, 0.1, 0.8],  # "Your"
    [0.5, 0.8, 0.6],  # "journey"
    [0.5, 0.8, 0.6],  # "starts"
])

# Compute attention scores
scores = torch.matmul(X, X.T)    # (T x T)
weights = F.softmax(scores, dim=-1)

# Compute new contextualized embeddings
Z = torch.matmul(weights, X)     # Weighted sum
print(Z)

#tensor([
# [0.4656, 0.5592, 0.6688],
# [0.4765, 0.6358, 0.6469],
# [0.4765, 0.6358, 0.6469]
#])

```


#### üß† **Causal Attention (d√πng trong GPT)** 
**D√πng ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªâ nh√¨n token ph√≠a tr∆∞·ªõc, kh√¥ng nh√¨n t∆∞∆°ng lai**  
M√¥ h√¨nh ch·ªâ ƒë∆∞·ª£c ph√©p ‚Äúd·ª± ƒëo√°n t·ª´ng token m·ªôt‚Äù ‚Üí **token by token generation**
Masking gi√∫p che nh·ªØng token ch∆∞a xu·∫•t hi·ªán

$$
\frac{Q \cdot K^\top}{\sqrt{d_k}} + \text{mask}
$$

![alt text](/images/image-01.png)


### Multi-Head Attention

Thay v√¨ d√πng 1 attention, d√πng nhi·ªÅu "heads" song song:

$$
 \text{MHA(Q,K,V)} = Concat(head1‚Äã,‚Ä¶,head n‚Äã) * Wo‚Äã
$$

![alt text](/images/image-02.png)

**L·ª£i √≠ch**: M·ªói head h·ªçc c√°c m·ªëi quan h·ªá kh√°c nhau (ng·ªØ ph√°p, ng·ªØ nghƒ©a, v.v.)
Each head learns different relationships:
 - syntax head (ng·ªØ ph√°p)
 - semantic head (nghƒ©a)
 - positional head (v·ªã tr√≠)


### T·ªïng k·∫øt
Attention gi√∫p m√¥ h√¨nh **t·∫≠p trung, suy lu·∫≠n v√† linh ho·∫°t**. ƒê√¢y l√† **core c·ªßa Transformer & GPT**.